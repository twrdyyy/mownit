{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filip Twardy MOwNiT lab 6\n",
    "\n",
    "Wyszukiwarka artykułów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dodatkowa informacja\n",
    "\n",
    "Aby dodatkowo sprawdziź jaki czas dana wyszukiwarka potrzebuje należy na początku komórki dodać wywołanie **%%time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowanie danych\n",
    "\n",
    "poniżej znajduje się autorski skrypt napisany w **bashu**, który generuje nam 1000 losowych artykułów z wikipedii, wystarczy zmienić wartość w pętli for aby zwiększyć ilość dokumentów. W tym notebooku użyłem 1000 ponieważ mój internet nie pozwala na wygenerowanie więcej w sensownym czasie (1000 dokumentów w około godzine)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "mkdir articles || echo \"Articles folder exists\"\n",
    "mkdir bags || echo \"Bags folder exists\"\n",
    "for i in {1..1000}\n",
    "do\n",
    "    article=$(lynx --dump https://en.wikipedia.org/wiki/Special:Random \\\n",
    "          tail -n +2 \\\n",
    "          sed '/^References/ q' \\\n",
    "          tr \"\\n%^&*()[]{},.\" \" \" \\\n",
    "          tr -s \" \")\n",
    " \n",
    "    topic=$(echo $article | awk '{print $1\" \"$2;}')\n",
    "    echo $article > ./articles/article$i.txt\n",
    "    echo \"Proceed $i articles with current topic $topic\"\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Słownik słów kluczowych i wektor cech ***bag-of-words***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T15:14:51.906505Z",
     "start_time": "2020-04-19T15:14:51.877583Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import string\n",
    "import os\n",
    "from pprint import pprint as pp\n",
    "\n",
    "BAGS_FOLDER = \"./bags/\"\n",
    "ARTICLES_FOLDER = \"./articles/\"\n",
    "\n",
    "log_article = lambda x: f\"Artykuł o nazwie -> {x[:-4]}\"\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"corpora/stopwords\")\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Zapisywanie i odczytywanie ***bag-of-words*** z pliku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T14:09:30.595225Z",
     "start_time": "2020-04-19T14:09:30.582692Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_bag_o_w(file_name, bag_of_words, text=\"\"):\n",
    "    \n",
    "    text = \"\\n\".join([\" \".join(list(map(str, item))) for item in bag_of_words.items()])\n",
    "    path = BAGS_FOLDER + file_name\n",
    "    \n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "        \n",
    "def load_bag_o_w(file_name):\n",
    "    \n",
    "    path = BAGS_FOLDER + file_name\n",
    "    bag_of_words = Counter()\n",
    "    \n",
    "    with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f.readlines():\n",
    "            key, val = line.replace(\"\\n\", \"\").split(\" \")\n",
    "            bag_of_words[key] = val\n",
    "              \n",
    "    return bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T14:59:42.415117Z",
     "start_time": "2020-04-19T14:59:42.407532Z"
    }
   },
   "source": [
    "##### Funkcja generująca ***bag-of-words***\n",
    "\n",
    "Funkcja generuje wektor na podstawie podanego teksu, zamienia wszystkie litery na małe, a następnie eliminuje wszystkie niepotrzebne słowa:\n",
    "- **english_stop_words** słowa, które w języku angielskim nie mają konkretnego znaczenia\n",
    "- **długość słowa** musi być większa niż 2\n",
    "\n",
    "Całość zapisujemy w strukturze **Counter** z modułu **collections**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T14:09:31.374719Z",
     "start_time": "2020-04-19T14:09:31.359814Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_bag_of_words(text):\n",
    "    \n",
    "    english_stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    text = text.lower() # wielkość liter nie ma znaczenia\n",
    "\n",
    "    bag_of_words = Counter([ # Couter zamienia liste na slownik wystąpień danego słowa\n",
    "        stemmer.stem(word) for sentence in sent_tokenize(text) # dzieli tekst na sentencje\n",
    "                           for word in word_tokenize(sentence) # dzieli sentencje na slowa\n",
    "                               if word not in english_stop_words \n",
    "                                   and len(word) > 2 # usuwa niepotrzebne slowa\n",
    "    ])\n",
    "    \n",
    "    return bag_of_words\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Przykładowe użycie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T15:00:41.010729Z",
     "start_time": "2020-04-19T15:00:40.979656Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'counter': 2,\n",
      "         'usual': 1,\n",
      "         'dictionari': 1,\n",
      "         'method': 1,\n",
      "         'avail': 1,\n",
      "         'object': 1,\n",
      "         'except': 1,\n",
      "         'two': 1,\n",
      "         'work': 1,\n",
      "         'differ': 1})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'usual': '1',\n",
       "         'dictionari': '1',\n",
       "         'method': '1',\n",
       "         'avail': '1',\n",
       "         'counter': '2',\n",
       "         'object': '1',\n",
       "         'except': '1',\n",
       "         'two': '1',\n",
       "         'work': '1',\n",
       "         'differ': '1'})"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words = get_bag_of_words(\"The usual dictionary methods are available for Counter objects except for two which work differently for counters.\")\n",
    "pp(bag_of_words)\n",
    "save_bag_o_w(\"test.txt\", bag_of_words)\n",
    "\n",
    "load_bag_o_w(\"test.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funkcja przetwarzająca wszystkie artykuły\n",
    "\n",
    "Funkcja przetważa każdy artykuł z folderu ***articles*** oraz tworzy słownik ***base*** wszystkich słów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T14:09:33.680533Z",
     "start_time": "2020-04-19T14:09:33.654306Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_articles():\n",
    "    base = Counter()\n",
    "    \n",
    "    _, _, articles = next(os.walk(ARTICLES_FOLDER))\n",
    "    N = len(articles)\n",
    "    get_file_name = lambda x: f\"article{x}.txt\"\n",
    "    \n",
    "    for i in range(1, N+1):\n",
    "        \n",
    "        file_name = get_file_name(i)\n",
    "        path = ARTICLES_FOLDER + file_name\n",
    "    \n",
    "        with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "\n",
    "            bag_of_words = get_bag_of_words(text=f.read())\n",
    "            base += bag_of_words\n",
    "            \n",
    "            save_bag_o_w(file_name=file_name,\n",
    "                         bag_of_words=bag_of_words)\n",
    "    \n",
    "    save_bag_o_w(file_name=\"base\", bag_of_words=base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcja generująca macierz ***term-by-document***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T14:09:18.504809Z",
     "start_time": "2020-04-19T14:09:18.493880Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_term_by_document_matrix(base_dict = \"base\"):\n",
    "    \n",
    "    terms = list(load_bag_o_w(base_dict))\n",
    "    \n",
    "    _, _, articles = next(os.walk(ARTICLES_FOLDER))\n",
    "    N, M = len(articles), len(terms)\n",
    "    \n",
    "    get_file_name = lambda x: f\"article{x}.txt\"\n",
    "    \n",
    "    tbd_matrix = sparse.lil_matrix((M, N))\n",
    "    \n",
    "    for i in range(N):\n",
    "        file_name = get_file_name(i+1)\n",
    "\n",
    "        bag_of_words = load_bag_o_w(file_name=file_name)\n",
    "        \n",
    "        for j, term in enumerate(terms):\n",
    "            tbd_matrix[j, i] = bag_of_words[term]\n",
    "            \n",
    "    return tbd_matrix.tocsr(), terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przetworzenie macierzy ***term-by-document*** metodą ***inverse document frequency***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T14:20:28.827156Z",
     "start_time": "2020-04-19T14:20:28.811795Z"
    }
   },
   "outputs": [],
   "source": [
    "def IDF_one_row(row):\n",
    "    N = row.shape[1]\n",
    "    n_w = row.count_nonzero()\n",
    "    return np.log(N / n_w)\n",
    "\n",
    "def IDF(tbd_matrix):\n",
    "    N = tbd_matrix.shape[0]\n",
    "    idf_matrix = np.zeros((N))\n",
    "    for i in range(N):\n",
    "        idf_matrix[i] = IDF_one_row(tbd_matrix[i,:])\n",
    "        tbd_matrix[i,:] *= idf_matrix[i]\n",
    "    return tbd_matrix, idf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wstępne przetworzenie\n",
    "\n",
    "Czas połączyć wszystko w całość, przetwarzamy wszystkie artykuły, ***bag-of-words*** każdego artykułu ląduje w folderze ***bags***. Następnie tworzymy macierz ***term-by-document*** i przetwarzamy ją metodą ***IDF***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T15:06:50.110077Z",
     "start_time": "2020-04-19T15:06:36.160467Z"
    }
   },
   "outputs": [],
   "source": [
    "process_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T15:09:22.967911Z",
     "start_time": "2020-04-19T15:06:50.111632Z"
    }
   },
   "outputs": [],
   "source": [
    "unprocessed_matrix, terms = get_term_by_document_matrix()\n",
    "idf_processed_matrix, idf_matrix = IDF(unprocessed_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T15:09:32.113304Z",
     "start_time": "2020-04-19T15:09:32.094767Z"
    }
   },
   "source": [
    "##### Prosta wyszukiwarka\n",
    "\n",
    "Pierwsza wyszukiwarka bazuje na korelacji między dwoma wektorami danej wzorem:\n",
    "    \n",
    "## $ cos(\\theta_j) = \\frac{q^T d_j}{||q||\\cdot||d_j||} = \\frac{q^T Ae_j}{||q||\\cdot||Ae_j||} $\n",
    "\n",
    "Wprowadzamy zapytanie **query** oraz liczbę **k** ile tekstów ma wyszukać w kolejności od najtrafniejszego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T14:44:20.493565Z",
     "start_time": "2020-04-19T14:44:20.446524Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_similar_articles_vanilla(query, tbd_matrix, k=20, base_dict=\"base\", idf_matrix=idf_matrix):\n",
    "    \n",
    "    terms = list(load_bag_o_w(base_dict))\n",
    "    _, _, articles = next(os.walk(ARTICLES_FOLDER))\n",
    "    N, M = len(articles), len(terms)\n",
    "    \n",
    "    get_file_name = lambda x: f\"article{x}.txt\"\n",
    "    articles = [get_file_name(x+1) for x in range(N)]\n",
    "    \n",
    "    q_vec = sparse.lil_matrix((M, 1))\n",
    "    q_bag = get_bag_of_words(query)\n",
    "\n",
    "    for i in range(M):\n",
    "        q_vec[i, 0] = q_bag[terms[i]] * idf_matrix[i]\n",
    "\n",
    "        \n",
    "    \n",
    "    q_norm =  sparse.linalg.norm(q_vec)\n",
    "    q_T = q_vec.transpose()\n",
    "    \n",
    "    if q_norm == 0:\n",
    "        return []\n",
    "    \n",
    "    ans = []\n",
    "    \n",
    "    for j in range(N):\n",
    "        d_j = tbd_matrix[:,j]\n",
    "        d_j_norm = sparse.linalg.norm(d_j)\n",
    "        \n",
    "        numerator = (q_T * d_j)[0,0] \n",
    "        denominator = (q_norm * d_j_norm)\n",
    "        \n",
    "        p = (numerator / denominator, articles[j])\n",
    "        ans.append(p)\n",
    "    \n",
    "    key_func = lambda x : -x[0]\n",
    "    ans_k = sorted(ans, key=key_func)[:k]\n",
    "    ans_k = list(map(lambda x: x[1], ans_k))\n",
    "    \n",
    "    return ans_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Przykładowe działanie\n",
    "\n",
    "Wyszukamy **5** artykułów o treści z gier fifa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T15:20:25.993248Z",
     "start_time": "2020-04-19T15:20:23.740821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wyszukiwarka Prosta \n",
      "-------------------------\n",
      "Szukaj -> 'fifa video game'???\n",
      "Znaleziono ->\n",
      "\n",
      "Artykuł o nazwie -> article402\n",
      "Artykuł o nazwie -> article591\n",
      "Artykuł o nazwie -> article454\n",
      "Artykuł o nazwie -> article530\n",
      "Artykuł o nazwie -> article751\n"
     ]
    }
   ],
   "source": [
    "query = \"fifa video game\"\n",
    "k = 5\n",
    "\n",
    "similar_articles = find_simillar_articles_vanilla(query=query,\n",
    "                                                  tbd_matrix=idf_processed_matrix,\n",
    "                                                  idf_matrix=idf_matrix,\n",
    "                                                  k=k)\n",
    "print(f\"\"\"Wyszukiwarka Prosta \n",
    "{\"-\" * 25}\n",
    "Szukaj -> '{query}'???\n",
    "Znaleziono ->\n",
    "\"\"\")\n",
    "print(\"\\n\".join(map(log_article, similar_articles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wyszukiwarka normalizująca\n",
    "\n",
    "Druga wyszukiwarka wykorzystuje ten sam wzór ale dodatkowo **normalizuje wektory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T14:36:19.271454Z",
     "start_time": "2020-04-19T14:36:19.235262Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_similar_articles_with_norm(query, tbd_matrix, k=20, base_dict=\"base\", idf_matrix=idf_matrix):\n",
    "    \n",
    "    terms = list(load_bag_o_w(base_dict))\n",
    "    _, _, articles = next(os.walk(ARTICLES_FOLDER))\n",
    "    N, M = len(articles), len(terms)\n",
    "    \n",
    "    get_file_name = lambda x: f\"article{x}.txt\"\n",
    "    articles = [get_file_name(x+1) for x in range(N)]\n",
    "    \n",
    "    q_vec = sparse.lil_matrix((M, 1))\n",
    "    q_bag = get_bag_of_words(query)\n",
    "\n",
    "    for i in range(M):\n",
    "        q_vec[i, 0] = q_bag[terms[i]] * idf_matrix[i]\n",
    "\n",
    "    for i in range(N):\n",
    "        norm = sparse.linalg.norm(tbd_matrix[:,i])\n",
    "        tbd_matrix[:,i] /= norm\n",
    "    \n",
    "    q_norm =  sparse.linalg.norm(q_vec)\n",
    "    q_T = q_vec.transpose()\n",
    "    q_T /= q_norm\n",
    "    \n",
    "    if q_norm == 0:\n",
    "        return []\n",
    "    \n",
    "    ans = []\n",
    "    \n",
    "    for j in range(N):\n",
    "        d_j = tbd_matrix[:,j]\n",
    "        d_j_norm = sparse.linalg.norm(d_j)\n",
    "        \n",
    "        numerator = (q_T * d_j)[0,0] \n",
    "        denominator = (q_norm * d_j_norm)\n",
    "        \n",
    "        p = (numerator / denominator, articles[j])\n",
    "        ans.append(p)\n",
    "    \n",
    "    key_func = lambda x : -x[0]\n",
    "    ans_k = sorted(ans, key=key_func)[:k]\n",
    "    ans_k = list(map(lambda x: x[1], ans_k))\n",
    "    \n",
    "    return ans_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Przykładowe działanie\n",
    "\n",
    "Wyszukamy **5** artykułów o treści z gier fifa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T15:20:40.201208Z",
     "start_time": "2020-04-19T15:20:32.940095Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wyszukiwarka Normalizująca \n",
      "-------------------------\n",
      "Szukaj -> 'fifa video game'???\n",
      "Znaleziono ->\n",
      "\n",
      "Artykuł o nazwie -> article402\n",
      "Artykuł o nazwie -> article591\n",
      "Artykuł o nazwie -> article454\n",
      "Artykuł o nazwie -> article530\n",
      "Artykuł o nazwie -> article751\n"
     ]
    }
   ],
   "source": [
    "query = \"fifa video game\"\n",
    "k = 5\n",
    "\n",
    "similar_articles = find_similar_articles_with_norm(query=query,\n",
    "                                                  tbd_matrix=idf_processed_matrix,\n",
    "                                                  idf_matrix=idf_matrix,\n",
    "                                                  k=k)\n",
    "\n",
    "print(f\"\"\"Wyszukiwarka Normalizująca \n",
    "{\"-\" * 25}\n",
    "Szukaj -> '{query}'???\n",
    "Znaleziono ->\n",
    "\"\"\")\n",
    "print(\"\\n\".join(map(log_article, similar_articles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wyszukiwarka SVD\n",
    "\n",
    "Trzecia wyszukiwarka wykorzystuje algorytm **symulowanego wyżarzania** wraz z **low rank approximation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T14:48:41.984869Z",
     "start_time": "2020-04-19T14:48:41.913459Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_similar_articles_with_svd(query, tbd_matrix, k=20,\n",
    "                                                       svd_k=100,\n",
    "                                                       base_dict=\"base\",\n",
    "                                                       idf_matrix=idf_matrix):\n",
    "    \n",
    "    terms = list(load_bag_o_w(base_dict))\n",
    "    _, _, articles = next(os.walk(ARTICLES_FOLDER))\n",
    "    N, M = len(articles), len(terms)\n",
    "    \n",
    "    get_file_name = lambda x: f\"article{x}.txt\"\n",
    "    articles = [get_file_name(x+1) for x in range(N)]\n",
    "    \n",
    "    q_vec = sparse.lil_matrix((M, 1))\n",
    "    q_bag = get_bag_of_words(query)\n",
    "\n",
    "    for i in range(M):\n",
    "        q_vec[i, 0] = q_bag[terms[i]] * idf_matrix[i]\n",
    "\n",
    "    \n",
    "    q_norm =  sparse.linalg.norm(q_vec)\n",
    "    q_T = q_vec.transpose()\n",
    "    \n",
    "    if q_norm == 0:\n",
    "        return []\n",
    "    \n",
    "    U, S, V = sparse.linalg.svds(tbd_matrix, svd_k)\n",
    "\n",
    "    U_k = sparse.lil_matrix(U[:, :svd_k])\n",
    "    S_k = sparse.lil_matrix(np.diag(S[:svd_k]))\n",
    "    V_k = sparse.lil_matrix(V[:svd_k, :])\n",
    "   \n",
    "    tbd_matrix_filtered = sparse.csc_matrix(U_k * S_k * V_k)\n",
    "    \n",
    "    ans = []\n",
    "    \n",
    "    for j in range(N):\n",
    "        d_j = tbd_matrix_filtered[:,j]\n",
    "        d_j_norm = sparse.linalg.norm(d_j)\n",
    "        \n",
    "        numerator = (q_T * d_j)[0,0] \n",
    "        denominator = (q_norm * d_j_norm)\n",
    "        \n",
    "        p = (numerator / denominator, articles[j])\n",
    "        ans.append(p)\n",
    "    \n",
    "    key_func = lambda x : -x[0]\n",
    "    ans_k = sorted(ans, key=key_func)[:k]\n",
    "    ans_k = list(map(lambda x: x[1], ans_k))\n",
    "    \n",
    "    return ans_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykładowe użycie \n",
    "\n",
    "Przykładowe użycia zostaną dokonane dla różnych wartości **k** dla **low rank approximation**\n",
    "- $k = 20, 50, 100, 200, 400$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T15:28:43.151212Z",
     "start_time": "2020-04-19T15:26:36.661253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Wyszukiwarka SVD dla k=20 \n",
      "-------------------------\n",
      "Szukaj -> 'fifa video game'???\n",
      "Znaleziono ->\n",
      "    \n",
      "Artykuł o nazwie -> article128\n",
      "Artykuł o nazwie -> article327\n",
      "Artykuł o nazwie -> article930\n",
      "Artykuł o nazwie -> article23\n",
      "Artykuł o nazwie -> article835\n",
      "-------------------------\n",
      "Wyszukiwarka SVD dla k=50 \n",
      "-------------------------\n",
      "Szukaj -> 'fifa video game'???\n",
      "Znaleziono ->\n",
      "    \n",
      "Artykuł o nazwie -> article751\n",
      "Artykuł o nazwie -> article892\n",
      "Artykuł o nazwie -> article933\n",
      "Artykuł o nazwie -> article128\n",
      "Artykuł o nazwie -> article990\n",
      "-------------------------\n",
      "Wyszukiwarka SVD dla k=100 \n",
      "-------------------------\n",
      "Szukaj -> 'fifa video game'???\n",
      "Znaleziono ->\n",
      "    \n",
      "Artykuł o nazwie -> article591\n",
      "Artykuł o nazwie -> article933\n",
      "Artykuł o nazwie -> article751\n",
      "Artykuł o nazwie -> article892\n",
      "Artykuł o nazwie -> article916\n",
      "-------------------------\n",
      "Wyszukiwarka SVD dla k=200 \n",
      "-------------------------\n",
      "Szukaj -> 'fifa video game'???\n",
      "Znaleziono ->\n",
      "    \n",
      "Artykuł o nazwie -> article591\n",
      "Artykuł o nazwie -> article933\n",
      "Artykuł o nazwie -> article892\n",
      "Artykuł o nazwie -> article751\n",
      "Artykuł o nazwie -> article492\n",
      "-------------------------\n",
      "Wyszukiwarka SVD dla k=400 \n",
      "-------------------------\n",
      "Szukaj -> 'fifa video game'???\n",
      "Znaleziono ->\n",
      "    \n",
      "Artykuł o nazwie -> article591\n",
      "Artykuł o nazwie -> article933\n",
      "Artykuł o nazwie -> article492\n",
      "Artykuł o nazwie -> article751\n",
      "Artykuł o nazwie -> article454\n"
     ]
    }
   ],
   "source": [
    "query = \"fifa video game\"\n",
    "K = [20, 50, 100 ,200 ,400]\n",
    "\n",
    "for k in K:\n",
    "    similar_articles = find_similar_articles_with_svd(query=query,\n",
    "                                                      tbd_matrix=idf_processed_matrix,\n",
    "                                                      idf_matrix=idf_matrix,\n",
    "                                                      k=5,\n",
    "                                                      svd_k=k)\n",
    "    print(f\"\"\"{\"-\" * 25}\n",
    "Wyszukiwarka SVD dla k={k} \n",
    "{\"-\" * 25}\n",
    "Szukaj -> '{query}'???\n",
    "Znaleziono ->\n",
    "    \"\"\")\n",
    "    print(\"\\n\".join(map(log_article, similar_articles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podsumowanie\n",
    "\n",
    "Wyszukiwarka wykorzystująca ***IDF*** oraz ***symulowane wyżarzanie*** najlepiej radzi sobie dla ***K=100***\n",
    "co pozwala nam znacząco zmniejszyć potrzebną pamięć na przetrzymywanie macierzy ***term-by-document***.\n",
    "\n",
    "Poniżej dokonam dodatkowo prezentacji jak algorytm działa bez preporcessingu **IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T15:32:45.908034Z",
     "start_time": "2020-04-19T15:32:43.562520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wyszukiwarka Prosta bez IDF\n",
      "-------------------------\n",
      "Szukaj -> 'fifa video game'???\n",
      "Znaleziono ->\n",
      "\n",
      "Artykuł o nazwie -> article402\n",
      "Artykuł o nazwie -> article591\n",
      "Artykuł o nazwie -> article454\n",
      "Artykuł o nazwie -> article530\n",
      "Artykuł o nazwie -> article751\n"
     ]
    }
   ],
   "source": [
    "query = \"fifa video game\"\n",
    "k = 5\n",
    "\n",
    "similar_articles = find_simillar_articles_vanilla(query=query,\n",
    "                                                  tbd_matrix=unprocessed_matrix, # <- tutaj\n",
    "                                                  idf_matrix=idf_matrix,\n",
    "                                                  k=k)\n",
    "print(f\"\"\"Wyszukiwarka Prosta bez IDF\n",
    "{\"-\" * 25}\n",
    "Szukaj -> '{query}'???\n",
    "Znaleziono ->\n",
    "\"\"\")\n",
    "print(\"\\n\".join(map(log_article, similar_articles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T15:32:54.885615Z",
     "start_time": "2020-04-19T15:32:47.717596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wyszukiwarka Normalizująca bez IDF\n",
      "-------------------------\n",
      "Szukaj -> 'fifa video game'???\n",
      "Znaleziono ->\n",
      "\n",
      "Artykuł o nazwie -> article402\n",
      "Artykuł o nazwie -> article591\n",
      "Artykuł o nazwie -> article454\n",
      "Artykuł o nazwie -> article530\n",
      "Artykuł o nazwie -> article751\n"
     ]
    }
   ],
   "source": [
    "query = \"fifa video game\"\n",
    "k = 5\n",
    "\n",
    "similar_articles = find_similar_articles_with_norm(query=query,\n",
    "                                                  tbd_matrix=unprocessed_matrix, # <- tutaj\n",
    "                                                  idf_matrix=idf_matrix,\n",
    "                                                  k=k)\n",
    "\n",
    "print(f\"\"\"Wyszukiwarka Normalizująca bez IDF\n",
    "{\"-\" * 25}\n",
    "Szukaj -> '{query}'???\n",
    "Znaleziono ->\n",
    "\"\"\")\n",
    "print(\"\\n\".join(map(log_article, similar_articles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T15:33:51.728565Z",
     "start_time": "2020-04-19T15:33:35.388027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Wyszukiwarka SVD dla k=100 bez IDF\n",
      "-------------------------\n",
      "Szukaj -> 'fifa video game'???\n",
      "Znaleziono ->\n",
      "    \n",
      "Artykuł o nazwie -> article591\n",
      "Artykuł o nazwie -> article933\n",
      "Artykuł o nazwie -> article751\n",
      "Artykuł o nazwie -> article892\n",
      "Artykuł o nazwie -> article916\n"
     ]
    }
   ],
   "source": [
    "query = \"fifa video game\"\n",
    "k = 100\n",
    "\n",
    "similar_articles = find_similar_articles_with_svd(query=query,\n",
    "                                                  tbd_matrix=unprocessed_matrix, # <- tutaj\n",
    "                                                  idf_matrix=idf_matrix,\n",
    "                                                  k=5,\n",
    "                                                  svd_k=k)\n",
    "print(f\"\"\"{\"-\" * 25}\n",
    "Wyszukiwarka SVD dla k={k} bez IDF\n",
    "{\"-\" * 25}\n",
    "Szukaj -> '{query}'???\n",
    "Znaleziono ->\n",
    "    \"\"\")\n",
    "print(\"\\n\".join(map(log_article, similar_articles)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
